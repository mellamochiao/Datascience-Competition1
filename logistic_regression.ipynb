{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_names=[]\n",
    "X_trains=[]\n",
    "y_trains=[]\n",
    "X_tests=[]\n",
    "base_path = Path(r\"C:/Users/athen/Downloads/Competition_data/Competition_data\")\n",
    "\n",
    "for folder_name in os.listdir(base_path):\n",
    "    dataset_names.append(folder_name)\n",
    "    X_trains.append(pd.read_csv(base_path / folder_name / \"X_train.csv\", header=0))\n",
    "    y_trains.append(pd.read_csv(base_path / folder_name / \"y_train.csv\", header=0))\n",
    "    X_tests.append(pd.read_csv(base_path / folder_name / \"X_test.csv\", header=0))\n",
    "## your code here\n",
    "def check_data_quality(df, dataset_name):\n",
    "    print(f\"\\n{dataset_name} 數據質量報告:\")\n",
    "    \n",
    "    # 缺失值檢查\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"缺失值統計:\")\n",
    "        print(missing[missing > 0])\n",
    "    \n",
    "    # 重複行檢查\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"重複行數量: {duplicates}\")\n",
    "    \n",
    "    # 基本統計信息\n",
    "    print(\"\\n數值型特徵基本統計:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # 異常值檢查 (以IQR方法為例)\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_cols:\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outliers = df[(df[col] < Q1 - 1.5*IQR) | (df[col] > Q3 + 1.5*IQR)]\n",
    "        if len(outliers) > 0:\n",
    "            print(f\"\\n{col} 列可能的異常值數量: {len(outliers)}\")\n",
    "def handle_missing_values(df):\n",
    "    # 數值型特徵用中位數填充\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numeric_cols:\n",
    "        df[col].fillna(df[col].median(), inplace=True)\n",
    "    \n",
    "    # 類別型特徵用眾數填充\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "def scale_features(X_train, X_test):\n",
    "    # 標準化\n",
    "    scaler = StandardScaler()\n",
    "    # 或使用 MinMaxScaler() 歸一化\n",
    "    # 或使用 RobustScaler() 處理異常值\n",
    "    \n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "    return X_train, X_test, scaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "def encode_features(X_train, X_test):\n",
    "    # 標籤編碼\n",
    "    le_encoders = {}\n",
    "    # 獲取類別型特徵\n",
    "    categorical_cols = X_train.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "        # 檢查是否有缺失值\n",
    "        if X_train[col].isnull().any():\n",
    "            X_train[col].fillna('Unknown', inplace=True)\n",
    "        if X_test[col].isnull().any():\n",
    "            X_test[col].fillna('Unknown', inplace=True)\n",
    "            \n",
    "        # 合併訓練和測試集的唯一值進行擬合\n",
    "        unique_values = pd.concat([X_train[col], X_test[col]]).unique()\n",
    "        le.fit(unique_values)\n",
    "        \n",
    "        # 轉換數據\n",
    "        X_train[col] = le.transform(X_train[col])\n",
    "        X_test[col] = le.transform(X_test[col])\n",
    "        le_encoders[col] = le\n",
    "    \n",
    "    return X_train, X_test, le_encoders\n",
    "def create_features(df):\n",
    "    # 數值特徵間的交互\n",
    "    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # 加法交互\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "            df[f'{col1}_plus_{col2}'] = df[col1] + df[col2]\n",
    "    \n",
    "    # 乘法交互\n",
    "    for i in range(len(numeric_cols)):\n",
    "        for j in range(i+1, len(numeric_cols)):\n",
    "            col1, col2 = numeric_cols[i], numeric_cols[j]\n",
    "            df[f'{col1}_mult_{col2}'] = df[col1] * df[col2]\n",
    "    \n",
    "    # 統計特徵\n",
    "    for col in numeric_cols:\n",
    "        df[f'{col}_squared'] = df[col] ** 2\n",
    "        df[f'{col}_cubed'] = df[col] ** 3\n",
    "        df[f'{col}_log'] = np.log1p(df[col] - df[col].min() + 1)\n",
    "    \n",
    "    return df\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def select_features(X_train, y_train, X_test, method='statistical', n_features=10):\n",
    "    if method == 'statistical':\n",
    "        # 使用 SelectKBest 和 f_classif\n",
    "        selector = SelectKBest(score_func=f_classif, k=n_features)\n",
    "        X_train_selected = selector.fit_transform(X_train, y_train)\n",
    "        X_test_selected = selector.transform(X_test)\n",
    "        \n",
    "        # 獲取選擇的特徵名稱\n",
    "        selected_features = X_train.columns[selector.get_support()].tolist()\n",
    "        \n",
    "    elif method == 'tree_based':\n",
    "        # 使用隨機森林特徵重要性\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        \n",
    "        # 獲取特徵重要性\n",
    "        importances = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        })\n",
    "        importances = importances.sort_values('importance', ascending=False)\n",
    "        \n",
    "        # 選擇前n個最重要特徵\n",
    "        selected_features = importances['feature'].head(n_features).tolist()\n",
    "        X_train_selected = X_train[selected_features]\n",
    "        X_test_selected = X_test[selected_features]\n",
    "    \n",
    "    return X_train_selected, X_test_selected, selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "model = LogisticRegression(\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    C=1.0,  # 調整這個值：較小的值增加正則化強度\n",
    "    penalty='l2',  # 可以改用 'l1' 進行特徵選擇\n",
    "    solver='lbfgs',  # 可以嘗試不同的優化器\n",
    "    class_weight='balanced'\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 訓練和評估模型\n",
    "print(\"\\n=== 模型訓練和驗證集評估 ===\")\n",
    "models = []\n",
    "validation_aucs = []  # 儲存每個模型的驗證集AUC\n",
    "\n",
    "for i in range(len(dataset_names)):\n",
    "    # 分割訓練集和驗證集\n",
    "    tmp_X_train, tmp_X_val, tmp_y_train, tmp_y_val = train_test_split(\n",
    "        X_trains[i], y_trains[i], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # 建立邏輯迴歸模型\n",
    "    model = LogisticRegression(\n",
    "        random_state=42,\n",
    "        max_iter=1000,  # 增加迭代次數以確保收斂\n",
    "        C=1.0,         # 正則化強度的倒數\n",
    "        class_weight='balanced'  # 處理類別不平衡\n",
    "    )\n",
    "    \n",
    "    # 訓練模型\n",
    "    model.fit(tmp_X_train, tmp_y_train.squeeze())\n",
    "    \n",
    "    # 在驗證集上評估\n",
    "    val_proba = model.predict_proba(tmp_X_val)[:, 1]\n",
    "    val_auc = roc_auc_score(tmp_y_val, val_proba)\n",
    "    validation_aucs.append(val_auc)\n",
    "    \n",
    "    print(f\"\\n{dataset_names[i]} 模型評估:\")\n",
    "    print(f\"- 驗證集 AUC: {val_auc:.4f}\")\n",
    "    \n",
    "    # 顯示特徵重要性\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X_trains[i].columns,\n",
    "        'Importance': abs(model.coef_[0])\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\n前5個最重要特徵:\")\n",
    "    print(feature_importance.head())\n",
    "    \n",
    "    models.append(model)\n",
    "\n",
    "# 顯示所有模型的平均表現\n",
    "print(f\"\\n=== 整體模型表現 ===\")\n",
    "print(f\"平均驗證集 AUC: {np.mean(validation_aucs):.4f}\")\n",
    "print(f\"最佳驗證集 AUC: {np.max(validation_aucs):.4f} (Dataset_{np.argmax(validation_aucs) + 1})\")\n",
    "print(f\"最差驗證集 AUC: {np.min(validation_aucs):.4f} (Dataset_{np.argmin(validation_aucs) + 1})\")\n",
    "\n",
    "# 進行預測和儲存結果\n",
    "print(\"\\n=== 預測和儲存進度 ===\")\n",
    "y_predicts = []\n",
    "for i in range(len(dataset_names)):\n",
    "    # 獲取預測概率\n",
    "    y_predict_proba = models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    \n",
    "    # 儲存預測結果\n",
    "    output_path = Path(base_path) / dataset_names[i] / \"y_predict.csv\"\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_csv(output_path, index=False, header=True)\n",
    "    \n",
    "    print(f\"\\n儲存 {dataset_names[i]} 的預測結果:\")\n",
    "    print(f\"- 驗證集 AUC: {validation_aucs[i]:.4f}\")\n",
    "    print(f\"- 預測數量: {len(df)} 筆\")\n",
    "    print(f\"- 預測值範圍: {df['y_predict_proba'].min():.3f} 到 {df['y_predict_proba'].max():.3f}\")\n",
    "    print(f\"- 儲存路徑: {output_path}\")\n",
    "\n",
    "# 模型表現比較\n",
    "print(\"\\n=== 模型表現排名 ===\")\n",
    "performance_df = pd.DataFrame({\n",
    "    'Dataset': dataset_names,\n",
    "    'Validation_AUC': validation_aucs\n",
    "})\n",
    "performance_df = performance_df.sort_values('Validation_AUC', ascending=False)\n",
    "print(\"\\n模型表現排名:\")\n",
    "print(performance_df)\n",
    "y_predicts=[]\n",
    "for i in range(len(dataset_names)):\n",
    "    y_predict_proba=models[i].predict_proba(X_tests[i])[:, 1]\n",
    "    df = pd.DataFrame(y_predict_proba, columns=['y_predict_proba'])\n",
    "    y_predicts.append(df)\n",
    "    print(\"\\n=== 數據集列表 ===\")\n",
    "for i, name in enumerate(dataset_names):\n",
    "    print(f\"{i+1}. {name}\")\n",
    "\n",
    "print(\"\\n=== 預測和儲存進度 ===\")\n",
    "for idx, dataset_name in enumerate(dataset_names):\n",
    "    df = y_predicts[idx]\n",
    "    output_path = Path(base_path) / dataset_name / \"y_predict.csv\"  \n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 儲存預測結果\n",
    "    df.to_csv(output_path, index=False, header=True)\n",
    "    \n",
    "    # 顯示儲存資訊\n",
    "    print(f\"\\n儲存 {dataset_name} 的預測結果:\")\n",
    "    print(f\"- 儲存路徑: {output_path}\")\n",
    "    print(f\"- 預測數量: {len(df)} 筆\")\n",
    "    print(f\"- 預測值範圍: {df['y_predict_proba'].min():.3f} 到 {df['y_predict_proba'].max():.3f}\")\n",
    "    print(f\"- 檔案大小: {output_path.stat().st_size / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n=== 最終檢查 ===\")\n",
    "all_success = True\n",
    "for dataset_name in dataset_names:\n",
    "    check_path = Path(base_path) / dataset_name / \"y_predict.csv\"\n",
    "    if check_path.exists():\n",
    "        print(f\"✓ {dataset_name} 預測檔案已成功儲存\")\n",
    "    else:\n",
    "        print(f\"✗ {dataset_name} 預測檔案未找到！\")\n",
    "        all_success = False\n",
    "\n",
    "if all_success:\n",
    "    print(\"\\n所有數據集的預測結果都已成功儲存！\")\n",
    "else:\n",
    "    print(\"\\n警告：部分數據集的預測結果可能未正確儲存，請檢查上述訊息。\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
